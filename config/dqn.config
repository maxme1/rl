import numpy as np
import torch
from torch import nn

from dpipe import layers
from dpipe.batch_iter import Infinite, zip_apply
from dpipe.train import Checkpoints
from rl_utils.training import sample_and_populate, HardUpdateAgent
from rl_utils.utils import HistLogger
from rl_utils.play import eps_greedy
from rl_utils.dqn import get_q_values

models = [
    nn.Sequential(
        nn.Conv2d(n_frames, 16, kernel_size=3),
        layers.ConsistentSequential(
            layers.PreActivation2d, [16, 16, 16], kernel_size=3, batch_norm_module=None),

        nn.MaxPool2d(2),
        layers.ConsistentSequential(
            layers.PreActivation2d, [16, 32, 32], kernel_size=3, batch_norm_module=None),

        nn.MaxPool2d(2),
        layers.ConsistentSequential(
            layers.PreActivation2d, [32, 64, 64], kernel_size=3, batch_norm_module=None),

        nn.MaxPool2d(2),
        layers.ConsistentSequential(
            layers.PreActivation2d, [64, 128, 128], kernel_size=3, batch_norm_module=None),

        nn.MaxPool2d(2),
        layers.ConsistentSequential(
            layers.PreActivation2d, [128, 256, 256], kernel_size=3, batch_norm_module=None),

        nn.AdaptiveAvgPool2d((1, 1)),
        nn.Flatten(),
        nn.Linear(256, 64),
        nn.ReLU(),
        nn.Linear(64, n_actions),
    ).cuda()
    for _ in range(2)
]

agent = models[0]
target = models[1]
frequency = 1000
optimizer = torch.optim.Adam(agent.parameters())
updater = HardUpdateAgent(agent, target, frequency)


@eps_greedy(eps, n_actions)
def play_step(state):
    return get_q_values(state, agent).argmax()


batch_iter = Infinite(
    sample_and_populate(make_env(), memory, play_step, n_steps + 1, wrap_episode),
    zip_apply(np.float32, np.int64, np.float32, bool),
    batch_size=20, batches_per_epoch=batches_per_epoch)

checkpoints = Checkpoints(__file__.parent / 'checkpoints', [*models, optimizer])
logger = HistLogger(__file__.parent / 'logs')
