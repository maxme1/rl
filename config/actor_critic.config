from .dqn import *

from rl_utils.dqn import wrap_dqn
from rl_utils.actor_critic import ActorCritic
import rl_utils.actor_critic as actor_critic

env_name = 'PongNoFrameskip-v4'
env = wrap_dqn(gym.make(env_name))

agent = ActorCritic(stack_size, env.action_space.n).cuda()
memory = FrameLimitMemory(1e5)

calculate_loss = actor_critic.calculate_loss(
    # lazy
    gamma=gamma,
    entropy_weight=.01,
    value_weight=1,
)
get_action = epsilon_greedy_action(
    # lazy
    get_eps=eps, sampler=env.action_space.sample, get_action=actor_critic.get_action
)
