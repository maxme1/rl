from functools import partial
import numpy as np
import torch
import gym
from torch import nn

from dpipe import layers
from dpipe.batch_iter import Infinite, zip_apply
from dpipe.train import Checkpoints, train, TimeProfiler
from dpipe.im.shape_ops import zoom_to_shape

from rl_utils.training import sample_and_populate, HardUpdateAgent
from rl_utils.utils import HistLogger
from rl_utils.dqn import get_q_values, q_update, describe_dqn, NoisyLinear
from rl_utils.envs import MaxAndSkip, FireReset, EpisodicLife, LambdaWrapper
from rl_utils.memory import ExpirationMemory
from rl_utils.wrappers import ChangeState, SlidingWindow


def make_env():
    def atari(state):
        gray = state.mean(-1)[30:, 8:-8]
        return zoom_to_shape(gray, (64, 64)).astype('uint8')

    return MaxAndSkip(FireReset(EpisodicLife(LambdaWrapper(
        gym.make(env_name), atari, np.sign)
    )), 4, 2)


wrap_episode = lambda episode: ChangeState(SlidingWindow(episode, n_frames), lambda x: np.float32(x / 255))

memory = ExpirationMemory(10_000)

n_actions = make_env().action_space.n
n_frames = 4
n_steps = 3
gamma = 0.99


def play_step(state):
    return get_q_values(state, agent).argmax()


models = [
    nn.Sequential(
        nn.Conv2d(n_frames, 16, kernel_size=3, stride=2),
        nn.ReLU(),
        nn.Conv2d(16, 32, kernel_size=3, stride=2),
        nn.ReLU(),
        nn.Conv2d(32, 64, kernel_size=3, stride=2),
        nn.ReLU(),
        nn.Conv2d(64, 256, kernel_size=3, stride=2),
        nn.ReLU(),

        nn.AdaptiveAvgPool2d((1, 1)),
        nn.Flatten(),
        NoisyLinear(256, n_actions, 0.5),
    ).cuda()
    for _ in range(2)
]

agent = models[0].train()
target = models[1].eval()
optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)
updater = HardUpdateAgent(agent, target, 5)

batch_iter = Infinite(
    sample_and_populate(make_env(), memory, play_step, n_steps + 1, 10, wrap_episode),
    zip_apply(np.float32, np.int64, np.float32, bool),
    batch_size=20, batches_per_epoch=1000)

checkpoints = Checkpoints(__file__.parent / 'checkpoints', [*models, optimizer])
logger = HistLogger(__file__.parent / 'logs')
train_agent = train(
    q_update, batch_iter, agent=agent, target_agent=target, updater=updater,
    checkpoints=checkpoints, logger=logger,
    optimizer=optimizer, n_epochs=3000, lr=1e-4, gamma=gamma,
    profiler=TimeProfiler(logger.logger),
    validate=partial(describe_dqn, memory, agent, gamma)
)
